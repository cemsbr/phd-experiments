#======================================================
# Mandatory settings
#======================================================

# Hadoop home
hibench.hadoop.home		/home/hadoop/hadoop/

# Spark home
hibench.spark.home		/home/spark/spark/

# HDFS master
hibench.hdfs.master		hdfs://hadoop0
# example: hdfs://<hdfs_name_node_ip>:<hdfs_name_node_port>

# Spark master
#   standalone mode: `spark://xxx:7077`
#   YARN mode: `yarn-client`
#   unset: fallback to `local[1]`
hibench.spark.master		spark://hadoop0:7077

#======================================================
# Not mandatory but important settings
#======================================================

# `hibench.hadoop.executable` is used to auto probe hadoop version and
# hadoop release, which is critical for further configurations. Most
# cases `hadoop` executable be placed under HADOOP_HOME/bin. However,
# in some cases such as CDH?/MR1, it must be explicitly defined:

#hibench.hadoop.executable	${hibench.hadoop.home}/bin-mapreduce1/hadoop

# `hibench.hadoop.version` will be auto probed according to
# `hibench.hadoop.executable` version information report. However, for
# CDH release, both `hadoop version` of MR1 and MR2 will produce same
# report, which can't probe right MR versions. You'll need to
# explicitly define MR versions here.

#hibench.hadoop.version		hadoop1

# `hibench.spark.version` is used to choose which sparkbench workload
# jar. Mostly situation it'll be auto probed. Please override if spark
# version is not probed correctly.
# Note, supported values: `spark1.2` or `spark1.3`

hibench.spark.version          spark1.4

#======================================================
# Optional settings
#======================================================

# Removed, except the required ones below
# default is 1G
spark.executor.memory 2G
# default is 1G
spark.driver.memory   1G
